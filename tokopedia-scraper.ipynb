{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f361eeea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Scraper' from 'scraper' (c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\scraper\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscraper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scraper\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Scraper' from 'scraper' (c:\\laragon\\bin\\python\\python-3.10\\lib\\site-packages\\scraper\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from scraper import Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d764b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default HEADERS\n",
    "HEADERS = {\n",
    "    'origin': 'https://www.tokopedia.com',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'accept': 'application/json, text/plain, */*',\n",
    "    'referer': 'https://www.tokopedia.com/p/laptop-aksesoris/laptop',\n",
    "    'authority': 'ace.tokopedia.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokopediaData:\n",
    "    def __init__(self, productId=None, title=None, description=None, price=0):\n",
    "        self.id = productId\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        self.price = price\n",
    "\n",
    "    def display(self):\n",
    "        print(f\"ProductID: {self.id}\")\n",
    "        print(f\"Title: {self.title}\")\n",
    "        print(f\"Description: {self.description}\")\n",
    "        print(f\"Price: {self.price}\")\n",
    "\n",
    "    def toList(self):\n",
    "        data = [\n",
    "            self.id,\n",
    "            self.title,\n",
    "            self.description,\n",
    "            self.price\n",
    "        ]\n",
    "        return data\n",
    "\n",
    "    def toDict(self):\n",
    "        data_dict = {\n",
    "            'id': self.id,\n",
    "            'title': self.title,\n",
    "            'description': self.description,\n",
    "            'price': self.price\n",
    "        }\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokopediaScraper(Scraper):\n",
    "    def __init__(self, headers=HEADERS, debug=False):\n",
    "        super(TokopediaScraper, self).__init__()\n",
    "        self.headers = {**self.headers, **headers}\n",
    "        self.url_products = [\n",
    "            'https://www.tokopedia.com/bennykoicikarang/lenovo-yoga-900',\n",
    "            'https://www.tokopedia.com/toptech/hp-14-cm0091au-amd-a4-9125-ssd-128gb-4gb-win10-joy-2'\n",
    "        ]\n",
    "        self.debug = debug\n",
    "        self.data_detail_products = []\n",
    "\n",
    "    def getDataByClass(self, content, name, as_text=True):\n",
    "        \"\"\"Get Data by Class Name\n",
    "        \"\"\"\n",
    "        data = content.find(attrs={'class': name})\n",
    "        if as_text and data:\n",
    "            return data.text.strip()\n",
    "        return data\n",
    "\n",
    "    def getDataByProp(self, content, name, as_text=True, strip=True):\n",
    "        \"\"\"Get Data by Prop Name\n",
    "        \"\"\"\n",
    "        data = content.find(attrs={'itemprop': name})\n",
    "        resp = None\n",
    "        if data:\n",
    "            if as_text and strip:\n",
    "                resp = data.text.strip()\n",
    "            elif as_text and not strip:\n",
    "                content = \"\".join([str(item).strip() for item in data.contents])\n",
    "                resp = content\n",
    "            else:\n",
    "                resp = data\n",
    "        return resp\n",
    "\n",
    "    def getDataByAttr(self, content, attr={}, type='value', default=None):\n",
    "        \"\"\" Get Data by Attr \"\"\"\n",
    "        data = content.find(attrs=attr)\n",
    "        resp = default\n",
    "        if data:\n",
    "            if type == 'value':\n",
    "                resp = data['value']\n",
    "            elif type == 'text':\n",
    "                resp = data.text.strip()\n",
    "            else:\n",
    "                resp = data\n",
    "        return resp\n",
    "\n",
    "    def getDataValue(self, content, name, selector=getDataByProp, as_text=True, strip=True):\n",
    "        \"\"\"Get Value Data.\n",
    "        \"\"\"\n",
    "        # TODO: Add option to use `selector` so it's make more flexible\n",
    "        data = self.getDataByProp(content, name, as_text, strip)\n",
    "        return data\n",
    "\n",
    "    def getListUrl(self, offset=0, limit=100):\n",
    "        \"\"\"Get List URLs from default category.\n",
    "\n",
    "        :param offset: Offset of category page\n",
    "        :param limit: Limit data to show\n",
    "        :return: list of (cleaned) product url\n",
    "        \"\"\"\n",
    "        url_category = f\"https://ace.tokopedia.com/search/product/v3?scheme=https&device=desktop&related=true&start={offset}&ob=23&source=directory&st=product&identifier=laptop-aksesoris_laptop&sc=289&rows={limit}&unique_id=7164647f8d9d4c9798da21c416b76558&safe_search=false\"\n",
    "        req = self.openURL(url_category)\n",
    "        urls = []\n",
    "        if req:\n",
    "            result = req.json()\n",
    "            products = result.get('data').get('products')\n",
    "            for product in products:\n",
    "                product_url = product.get('url')\n",
    "                if product_url:\n",
    "                    cleaned_url = product_url.split('?')[0]\n",
    "                    urls.append(cleaned_url)\n",
    "        return urls\n",
    "\n",
    "    def setListUrl(self, urls):\n",
    "        \"\"\"Set List of URL Product to scrape \"\"\"\n",
    "        if isinstance(urls, list) or isinstance(urls, tuple):\n",
    "            self.url_products = urls\n",
    "        else:\n",
    "            print(\"[-] URL is not valid list or tuple!\")\n",
    "            # Set to empty if urls not valid\n",
    "            self.url_products = []\n",
    "\n",
    "    def extractProductDetail(self, url):\n",
    "        soup = self.getPage(url)\n",
    "        tokped = TokopediaData()\n",
    "        tokped.id = self.getDataByAttr(soup, attr={'name': 'product_id'}, default=0)\n",
    "        tokped.title = self.getDataValue(soup, 'name')\n",
    "        tokped.description = self.getDataValue(soup, 'description', strip=False)\n",
    "        tokped.price = self.getDataByAttr(soup, {'id': 'product_price_int'})\n",
    "\n",
    "        if self.debug:\n",
    "            tokped.display()\n",
    "\n",
    "        return tokped\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run Scraper \"\"\"\n",
    "        products = []\n",
    "        for url in self.url_products:\n",
    "            product = self.extractProductDetail(url)\n",
    "            products.append(product.toList())\n",
    "        self.data_detail_products = products\n",
    "\n",
    "    def saveData(self, path='data_products.csv'):\n",
    "        \"\"\"Save data products to file \"\"\"\n",
    "        with open(path, 'w') as file:\n",
    "            writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "            writer.writerow(['id', 'product_id', 'title', 'description', 'price'])\n",
    "            index = 0\n",
    "            for data in self.data_detail_products:\n",
    "                index += 1\n",
    "                writer.writerow([index, data[0], data[1], data[2], data[3]])\n",
    "\n",
    "            print(f\"Data saved to `{path}`\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
