{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "     ---------------------------------------- 0.0/143.0 kB ? eta -:--:--\n",
      "     ----------------------------- -------- 112.6/143.0 kB 2.2 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 112.6/143.0 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 143.0/143.0 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/4c/f3/038b302fdfbe3be7da016777069f26ceefe11a681055ea1f7817546508e3/soupsieve-2.5-py3-none-any.whl.metadata\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_path = 'C:/Users/LENOVO/AppData/Local/Google/Chrome/Application/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "options= webdriver.FirefoxOptions()\n",
    "options.add_argument(\"--headless\")  # Run the browser in headless mode\n",
    "\n",
    "\n",
    "# Create a new instance of the Firefox driver\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    # driver = webdriver.Firefox(executable_path='path/to/geckodriver')\n",
    "    driver.get(url)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Scrap datas from 10 pages\n",
    "    for page in range(10):\n",
    "\n",
    "        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.CSS_SELECTOR, '#zeus-root')))\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Scroll the page until the end of the page\n",
    "        if page == 10:\n",
    "            scroll = 11\n",
    "        else:\n",
    "            scroll = 12\n",
    "\n",
    "        for i in range(scroll):\n",
    "            driver.execute_script('window.scrollBy(0,500)')\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Parse the page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Scrap website pages\n",
    "        for item in soup.find_all('div', class_='css-974ipl'):\n",
    "            # Scrap product names and titles\n",
    "            product_name = item.find('div', class_='prd_link-product-name css-3um8ox').text\n",
    "            price = item.find('div', class_='prd_link-product-price css-1ksb19c').text\n",
    "\n",
    "            # Check if there is any rating or not\n",
    "            rates = item.find_all('span', class_='prd_rating-average-text css-t70v7i')\n",
    "            if len(rates) > 0:\n",
    "                rate = item.find('span', class_='prd_rating-average-text css-t70v7i').text\n",
    "            else:\n",
    "                rate = ''\n",
    "\n",
    "            # Check if there is any sold item or not\n",
    "            sold_items = item.find_all('span', class_='prd_label-integrity css-1duhs3e')\n",
    "            if len(sold_items) > 0:\n",
    "                sold = item.find('span', class_='prd_label-integrity css-1duhs3e').text\n",
    "            else:\n",
    "                sold = 0\n",
    "\n",
    "            # Scrap address details\n",
    "            for item2 in item.find_all('div', class_='css-1rn0irl'):\n",
    "                try:\n",
    "                    location = item2.find('span', class_='prd_link-shop-loc css-1kdc32b flip').text\n",
    "                except AttributeError:\n",
    "                    location = ''\n",
    "                try:\n",
    "                    seller = item.find('span', class_='prd_link-shop-name css-1kdc32b flip').text\n",
    "                except AttributeError:\n",
    "                    seller = ''\n",
    "\n",
    "                data.append(\n",
    "                    {\n",
    "                        'Penjual': seller,\n",
    "                        'Lokasi': location,\n",
    "                        'Produk': product_name,\n",
    "                        'Harga': price,\n",
    "                        'Rate': rate,\n",
    "                        'Tejual': sold\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        time.sleep(1)\n",
    "        driver.find_element(By.CSS_SELECTOR, 'button[aria-label^=\"Laman berikutnya\"]').click()\n",
    "        time.sleep(2)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.tokopedia.com/xionco/eorde-sofa-bed-futon-sleeper-minimalis-xionco-hitam?extParam=ivf%3Dtrue&src=topads'\n",
    "# data = get_data(url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv('dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_link(driver, url):\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slowly_scroll(driver,n):\n",
    "    y = 1000\n",
    "    for timer in range(0,n):\n",
    "         driver.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "         y += 1000  \n",
    "         time.sleep(1)\n",
    "\n",
    "def close_link(driver):\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_title():\n",
    "    path = '//*[@id=\"pdp_comp-product_content\"]/div/h1'\n",
    "    element = driver.find_element_by_xpath(path)\n",
    "    title = element.text\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_review():\n",
    "    path = '//*[@id=\"pdp_comp-review\"]/h2[4]'\n",
    "    element = driver.find_element_by_xpath(path)\n",
    "    text = str(element.text)\n",
    "    num = text.split()[-1]\n",
    "    num = num.replace(\"(\",\"\")\n",
    "    num = num.replace(\")\",\"\")\n",
    "    return num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def total_pages():\n",
    "    path = '//*[@id=\"pdp_comp-review\"]/div[15]/div/div/button[10]'\n",
    "    element = driver.find_element_by_xpath(path)\n",
    "    text = str(element.text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_one_review(path):\n",
    "    element = driver.find_element_by_xpath(path)\n",
    "    text = str(element.text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def click_next(driver):\n",
    "    driver.implicitly_wait(5)\n",
    "    #path = '//*[@id=\"header-main-wrapper\"]/div[5]/nav/div/div[2]/div/div[2]'\n",
    "    path = '//*[@id=\"pdp_comp-review\"]/div[15]/div/div/button[11]'\n",
    "    driver.find_element_by_xpath(path).click()\n",
    "    driver.implicitly_wait(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# link = input_link()\n",
    "# driver = chrome_driver()\n",
    "open_link(driver,url)\n",
    "slowly_scroll(driver,12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#scraping info\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m title \u001b[39m=\u001b[39m scrap_title()\n\u001b[0;32m      3\u001b[0m review_num \u001b[39m=\u001b[39m total_review()\n\u001b[0;32m      4\u001b[0m pages \u001b[39m=\u001b[39m total_pages()\n",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m, in \u001b[0;36mscrap_title\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrap_title\u001b[39m():\n\u001b[0;32m      2\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m//*[@id=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpdp_comp-product_content\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]/div/h1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     element \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element_by_xpath(path)\n\u001b[0;32m      4\u001b[0m     title \u001b[39m=\u001b[39m element\u001b[39m.\u001b[39mtext\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m title\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#scraping info\n",
    "title = scrap_title()\n",
    "review_num = total_review()\n",
    "pages = total_pages()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(title)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(review_num)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(pages)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'title' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(title)\n",
    "print(review_num)\n",
    "print(pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display scrap info\n",
    "print('Product title: ' + str(title))\n",
    "print('Total review ' + str(review_num))\n",
    "print('Total pages ' + str(pages))\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last page num\n",
    "last = int(review_num)%int(pages)\n",
    "print(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #last page num\n",
    "last = int(review_num)%10\n",
    "print(last)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap review in one review page\n",
    "reviews = []\n",
    "for i in range(10):\n",
    "    i += 5\n",
    "    review = scrap_one_review('//*[@id=\"pdp_comp-review\"]/div[' + str(i) + ']/div/div[2]/p/span')\n",
    "    reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_next(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    i += 5\n",
    "    review = scrap_one_review('//*[@id=\"pdp_comp-review\"]/div[' + str(i) + ']/div/div[2]/p/span')\n",
    "    reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
